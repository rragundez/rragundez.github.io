{"pages":[{"title":"Keras: multi-label classification with ImageDataGenerator","text":"This post was originally published in the GoDataDriven blog Multi-label classification is a useful functionality of deep neural networks. I recently added this functionality into Keras' ImageDataGenerator in order to train on data that does not fit into memory. This blog post shows the functionality and runs over a complete example using the VOC2012 dataset. Shut up and show me the code! Images taken in the wild are extremely complex. In order to really \"understand\" an image there are many factors that play a role, like the amount of objects in the image, their dynamics, the relation between frames, the positions of the objects, etc. In order to make AI capable of understanding images in the wild as we do, we must empower AI with all those capabilities. This empowerment may come in different ways, such like multi-class classification, multi-label classification, object detection (bounding boxes), segmentation, pose estimation, optical flow, etc. After a small discussion with collaborators of the keras-preprocessing package we decided to start empowering Keras users with some of these use cases through the known ImageDataGenerator class. In particular, thanks to the flexibility of the DataFrameIterator class added by @Vijayabhaskar this should be possible. Then, during our last GDD Friday at GoDataDriven I decided to go ahead and start adding the multi-class classification use case. 1 The end result was this PR . But first... What is multi-label classification? Not to be confused with multi-class classification, in a multi-label problem some observations can be associated with 2 or more classes. NOTE This functionality has just been released in PyPI yesterday in the keras-preprocessing 1.0.6 version. You can update keras to have the newest version by: pip install -U keras Multi-class classification in 3 steps In this part will quickly demonstrate the use of ImageDataGenerator for multi-class classification. 1. Image metadata to pandas dataframe Ingest the metadata of the multi-class problem into a pandas dataframe. The labels for each observation should be in a list or tuple. The filenames of the images can be ingested into the dataframe in two ways as shown in the image below. Relative paths: If you only state the filenames of the images you will have to use the directory argument later on when calling the method flow_from_dataframe . Absolute paths: In this case you can ditch the directory argument. 2 2. Instantiate DataFrameIterator Create the generator of the images batches. This is done by instantiating DataFrameIterator via the flow_from_dataframe method of ImageDataGenerator . Supposing we ingested the filenames as relative paths, the simplest instantantiation would look like this: 3 from keras.preprocessing.image import ImageDataGenerator img_iter = ImageDataGenerator () . flow_from_dataframe ( img_metadata_df , directory = '/home/rodrigo/.keras/datasets' , x_col = 'filename' , y_col = 'labels' , class_mode = 'categorical' ) The actual logic of creating the batches and handling data augmentation is managed by the DataFrameIterator class. You can look up other available arguments in here . 3. Train the model Train the model using the fit_generator method. 4 model . fit_generator ( img_iter ) This will yield batches directly from disk, allowing you to train on much more data than it can fit in your memory. That's it! 5 Rundown example with VOC2012 In this part I'll walk you through a multi-class classification problem step by step. The example will use the VOC2012 dataset which consist of ~17,000 images and 20 classes. Just by looking at the images below you can quickly observe that this is a quite diverse and difficult dataset. Perfect! The closer to a real-life example the better. Let's start by downloading the data into ~/.keras/datasets from here . ~/. keras / datasets / VOC2012 â”œâ”€â”€ Annotations â”‚ â”œâ”€â”€ 2010 _000002 . xml â”‚ â”œâ”€â”€ 2010 _000003 . xml â”‚ â”œâ”€â”€ 2011 _000002 . xml â”‚ â””â”€â”€ ... â”œâ”€â”€ ImageSets â”‚ â”œâ”€â”€ Action â”‚ â”œâ”€â”€ Layout â”‚ â”œâ”€â”€ Main â”‚ â””â”€â”€ Segmentation â”œâ”€â”€ JPEGImages â”‚ â”œâ”€â”€ 2010 _000002 . jpg â”‚ â”œâ”€â”€ 2010 _000003 . jpg â”‚ â”œâ”€â”€ 2011 _000002 . jpg â”‚ â””â”€â”€ ... â”œâ”€â”€ SegmentationClass â”‚ â”œâ”€â”€ 2010 _000002 . png â”‚ â”œâ”€â”€ 2010 _000003 . png â”‚ â””â”€â”€ 2011 _000003 . png â””â”€â”€ SegmentationObject â”œâ”€â”€ 2010 _000002 . png â”œâ”€â”€ 2010 _000003 . png â””â”€â”€ ... We will use the Annotations directory to extract the images metadata. Each image can also have repeated associated labels, the argument unique_labels of the function below regulates if we keep repeated labels. We will not, trust me, the problem is hard enough. import xml.etree.ElementTree as ET from pathlib import Path def xml_to_labels ( xml_data , unique_labels ): root = ET . XML ( xml_data ) labels = set () if unique_labels else [] labels_add = labels . add if unique_labels else labels . append # speeds up method lookup for i , child in enumerate ( root ): if child . tag == 'filename' : img_filename = child . text if child . tag == 'object' : for subchild in child : if subchild . tag == 'name' : labels_add ( subchild . text ) return img_filename , list ( labels ) def get_labels ( annotations_dir , unique_labels = True ): for annotation_file in annotations_dir . iterdir (): with open ( annotation_file ) as f : yield xml_to_labels ( f . read (), unique_labels ) annotations_dir = Path ( '~/.keras/datasets/VOC2012/Annotations' ) . expanduser () img_metadata = pd . DataFrame ( get_labels ( annotations_dir ), columns = [ 'filename' , 'labels' ]) After extraction we end up with a dataframe with relative paths as shown below. The filenames are then relative to images_dir = Path ( '~/.keras/datasets/VOC2012/JPEGImages' ) . expanduser () Scan the dataset Let's now have a quick look at how the labels are distributed accross the dataset. These counts can be easily be computed with a Counter object. from collections import Counter labels_count = Counter ( label for lbs in img_metadata [ 'labels' ] for label in lbs ) From here we can easily compute the class_weights for later use. total_count = sum ( labels_count . values ()) class_weights = { cls : total_count / count for cls , count in labels_count . items ()} Let's now plot the labels count. No bueno, no bueno at all! There are two types of imbalances in the dataset. Imbalance across different classes, and imbalance between positive and negative examples in some classes. The former imbalance type can produce overfitting to highly represented classes, person in this case. The latter imbalance type can produce that a class is always flagged as negative i.e. if cow will always be flagged negative this will yield a 97% accuracy on that class. So what can we do about it?... pray. I won't go into detail but one way to counter the imbalances is with a combination of class weights and sample weights. 6 Next step is to look at the shape and size distribution across the different images. As illustrated above, the dataset contains images of different heights and widths. I won't go into detail, but this is not really a problem if at the end of the feature extraction via convolutional layers a global pooling layer is applied. Unfortunately, there is another problem, when using flow_from_dataframe all images need to be standardized to the same width and height. 7 This is specified via the target_size parameter. The lower histogram plot is good to have because it can give us an approximate indication of the maximum batch and queue size our memory can fit when using the generator. In this example, I don't really use the plot though. Training the model First, we need to instantiate the ImageDataGenerator . I'll do this with a simple setup just normalizing the pixel values. I also included a validation split to use it for validation stats during training after each epoch. img_gen = ImageDataGenerator ( rescale = 1 / 255 , validation_split = 0.2 ) We can now create the training and validation DataFrameIterator by specifying subset as \"training\" or \"validation\" respectively. In the case of multi-label classification the class_mode should be \"categorical\" (the default value). img_iter = img_gen . flow_from_dataframe ( img_metadata , shuffle = True , directory = images_dir , x_col = 'filename' , y_col = 'labels' , class_mode = 'categorical' , target_size = ( 128 , 128 ), batch_size = 20 , subset = 'training' ) img_iter_val = img_gen . flow_from_dataframe ( img_metadata , shuffle = False , directory = images_dir , x_col = 'filename' , y_col = 'labels' , class_mode = 'categorical' , target_size = ( 128 , 128 ), batch_size = 200 , subset = 'validation' ) I will use the ResNet50 pre-trained model in this example. I will replace the last fully connected layers of the network by an output layer with 20 neurons, one for each class. 8 In addition, pay attention to the output activation function, I won't go into detail, but for multi-class classification the probability of each class should be independent, hence the use of the sigmoid function and not the softmax function which is used for multi-class problems. base_model = ResNet50 ( include_top = False , weights = 'imagenet' , input_shape = None , pooling = 'avg' ) for layer in base_model . layers : layer . trainable = False predictions = Dense ( 20 , activation = 'sigmoid' )( base_model . output ) model = Model ( inputs = base_model . input , outputs = predictions ) Next we compile the model using \"binary_crossentropy\" loss. Why binary cross-entropy and not categorical cross-entropy you ask? well, again, I won't go into detail, but if you use categorical_crossentropy you are basically not penalizing for false positives (if you are more of a code person than a math person here you go ). model . compile ( loss = 'binary_crossentropy' , optimizer = 'adam' ) NOTE : Even though I just said that for multi-label the math dictates sigmoid and binary cross-entropy, there are cases out there where softmax and categorical cross-entropy worked better. Like this one . Train the model already! Not yet... patience, \" lento pero seguro \". Let's talk about metrics for a multi-label problem like this. I hope it is obvious that accuracy is not the way to go. Instead, let's use f1_score , recall_score and precision_score . There is a slight problem though, yes life is a bitch, these metrics were removed from the keras metrics with a good reason . The correct way to implement these metrics is to write a callback function that calculates them at the end of each epoch over the validation data. Something like this: from itertools import tee # finally! I found something useful for it from sklearn import metrics class Metrics ( Callback ): def __init__ ( self , validation_generator , validation_steps , threshold = 0.5 ): self . validation_generator = validation_generator self . validation_steps = validation_steps or len ( validation_generator ) self . threshold = threshold def on_train_begin ( self , logs = {}): self . val_f1_scores = [] self . val_recalls = [] self . val_precisions = [] def on_epoch_end ( self , epoch , logs = {}): # duplicate generator to make sure y_true and y_pred are calculated from the same observations gen_1 , gen_2 = tee ( self . validation_generator ) y_true = np . vstack ( next ( gen_1 )[ 1 ] for _ in range ( self . validation_steps )) . astype ( 'int' ) y_pred = ( self . model . predict_generator ( gen_2 , steps = self . validation_steps ) > self . threshold ) . astype ( 'int' ) f1 = metrics . f1_score ( y_true , y_pred , average = 'weighted' ) precision = metrics . precision_score ( y_true , y_pred , average = 'weighted' ) recall = metrics . recall_score ( y_true , y_pred , average = 'weighted' ) self . val_f1_scores . append ( f1 ) self . val_recalls . append ( recall ) self . val_precisions . append ( precision ) print ( f \" - val_f1_score: {f1:.5f} - val_precision: {precision:.5f} - val_recall: {recall:.5f}\" ) return Finally! we are ready to train the model. FYI: I did little to no effort to optimize the model. metrics = Metrics ( img_iter_val , validation_steps = 50 ) history = model . fit_generator ( img_iter , epochs = 10 , steps_per_epoch = 250 , class_weight = class_weights , callbacks = [ metrics ] ) During the training time you should see validation metrics at the end of each epoch, something like this: Epoch 10/10 250/250 [==============================] - 261s 1s/step - loss: 5.0277 - val_f1_score: 0.28546 - val_precision: 0.21283 - val_recall: 0.58719 If your memory melts during training reduce the batch_size , the target_size or the max_queue_size parameters. Post-mortem investigation In the case of a multi-class problem, it is already of big help to plot the confusion matrix, in that way we can identify very clearly where the model is \"confusing\" one class for another and address the problems directly. Due to the multi-label nature of the problem makes no sense to do the same. Instead a confusion matrix per class can be reviewed. 9 This functionality is only in the development version of scikit-learn , you can get that version by pip install git+https://www.github.com/scikit-learn/scikit-learn.git --upgrade after that you should be able to from sklearn.metrics import multilabel_confusion_matrix I wrote a wrapper plot function plot_multiclass_confusion_matrix around multilabel_confusion_matrix , which you can find in the code . The output from it looks like this: That's it folks! As you can see the model sucks. Your mission, should you choose to accept it... Adios I hope you found this blog post useful. I went through many concepts rather quickly but I think there are some valuable tips in there. You can find the code here . If you have any other questions just ping me on twitter @rragundez . This was possible before but in a hacky not very API friendly way. You can read about it here . â†© Tha absolute path format gives you more flexibility as you can build a dataset from several directories. â†© In the case of multi-class classification make sure to use class_mode='categorical' . â†© For multi-class classification make sure the output layer of the model has a sigmoid activation function and that the loss function is binary_crossentropy . â†© I hope you appreciate the simplicity of it :) â†© Sample weights are not yet implemented in flow_from_dataframe . I'm waiting on this person , but if you would like to contribute please do! â†© This is a requirement because each batch of images is loaded into a numpy array, therefore each loaded image should have the same array dimensions. Moreover, this will be a great feature to have, a PR would be quite cumbersome though, but go for it! â†© The output layer from ResNet50 if include_top=False has size 2048, I wouldn't normally followed with a fully connected layer of 20 neurons, but for this example is sufficient to show functionality. Normally I try dropping the output units by 1/3 on every layer or 1/10 if 1/3 is not sufficient. â†© There are several things that a confusion matrix per class will miss but it's a good first approach. â†©","tags":"tech","url":"/keras-multi-label.html"},{"title":"Big Data Expo 2018: Deep Learning, the Engine of the AI Revolution","text":"This post was originally published in the GoDataDriven blog Two weeks ago I had the opportunity to present a talk about Deep Learning at the Big Data Expo 2018 . This was a non-technical talk trying to demystify Deep Learning and explain why Deep Learning is driving the AI tech revolution forward. 1 The title of my talk was Deep Learning, the Engine of the AI Revolution , and when I say revolution I don't mean just an advance in technology but a technology that changes society. This blog post is an attempt to summarize my talk. It's a bit choppy because the talk had a lot of storytelling to link concepts together, but I hope this post still delivers some value for those not present during the talk. 2 Tech revolutions In the late 90s/beginning of the 00s we had the internet revolution where companies like Google, Facebook, Youtube, Netflix, and Airbnb started. We also saw some companies adapt to the internet era and release new products such as MSN Messenger, Hotmail, and the iTunes store. Then, after the release of the iPhone in 2007, the mobile revolution started. Great wealth was created with users engaging much more with products through mobile applications. Companies like Uber, Tinder, Snapchat, and Instagram were created. Savvy companies from the internet era established dominance by launching mobile applications like Facebook, YouTube, LinkedIn, and Spotify. The next wave will be the artificial intelligence revolution. AI is ready to create disruptions in different sectors and markets, in some cases, it has driven new products already, like Google Translate or LG Cloi . Technology is changing the customer experience because recently AI has been able to take a stab at problems that relate to human interactions, problems related to text, speech, audio, vision, coordination and understanding of the environment. Everyone is familiar with the big companies like Google ( Waymo ) and Tesla, racing to create the perfect self-driving car using AI, and with smart assistants like Alexa, but AI powered by deep learning is also embedded in many other sectors through smaller companies. Take Lyrebird for example, a company based in Canada which can synthesize your voice, or Viecure and MedicX.AI which bring AI to the medical sector, or CaptainAI trying to disrupt the boat and ship industry. There are many more examples, but the wave is clearly coming and my question to you is: is your organization prepared to adapt to this new AI era? How does deep learning fit in the artificial intelligence ecosystem? Let's start by looking at the concepts of artificial intelligence, machine learning, and deep learning. Artificial intelligence as defined by John McCarthy, who coined the term in 1956, is \"the science and engineering of making intelligent machines\". Notice that there is no specification on how machines mimic intelligence; it can be done through business rules, brute-force algorithms, symbol manipulation, or statistical learning. Machine learning is a concept within artificial intelligence. Arthur Samuel defined the term machine learning in 1959 as \"the ability to learn without being explicitly programmed\". The main idea is that data goes into a learning algorithm and out comes a model which is able to perform predictions given new, as-yet-unseen data. Deep learning is a concept within machine learning and artificial intelligence. It uses machine learning algorithms inspired by the human brain, those containing the concepts of neuron or synapses. An example of artificial intelligence but not machine learning is a tax calculator for example, which performs a task based on business rules. Another example is the famous Deep Blue which defeated the chess champion Garry Kasparov in 1996 by using an brute-force algorithm. The old approach to solving spam email which was based on the Naive Bayes technique is an example of machine learning and not deep learning; this I'll call traditional machine learning. In summary, this approach counts the number of times a word appears in a corpus of known spam emails and assigns a probability of being spam if a new email contains this word. Another example of traditional machine learning is applying a model (e.g. Random Forest) to a number of engineered features to make predictions (for example, predicting house prices based on floor area, number of rooms, location, etc.) A deep learning example is Smart Reply , which was introduced in Gmail in 2017. Smart Reply is able to understand what is being said in the email and suggest a human-like response which matches the context of the conversation. Another example of deep learning is the identification of the subject in a description or review, or the recent poster boy of deep learning, AlphaGo . Traditional ML vs deep learning Traditional machine learning requires the creation of features. This is normally done by a data scientist in collaboration with a domain expert. These features need to be chosen or hardcoded by a human. On the other hand, deep learning does not require the creation of human-engineered features. The creation of features happens within the learning algorithm itself. One of the advantages of using deep learning is that for use cases where is not easy to create or identify which features to use; the algorithm will figure them out for you. However, one of the drawbacks is that these features will not necessarily be interpretable by humans, therefore making difficult to explain how the model is working and why itis making certain decisions. 3 Do you remember the graph below? With the promise of the power of unstructured data, companies have put a lot of effort and resources into setting state-of-the-art environments and data pipelines. Deep learning is the correct tool to use the full capabilities of unstructured data, if you are using traditional machine learning and creating features on top of unstructured data you are basically turning it into structured, kneecapping the potential value of the unstructured data pipelines. As unstructured data becomes dominant, deep learning is a must in the toolbox. Another advantage of using deep learning models is that, as the amount of data grows, the models keep on learning and are able to capture more complex relations within the data. DL ecosystem Supervised learning is probably the most well-known topic within machine learning and it relates to having a labeled dataset to train on. Semi-supervised learning as the name indicates relates to the situation when we have only part of our dataset labeled. Unsupervised learning is then when the complete dataset has no labels, and most of the time the task is not so much to predict something but to find relations in our data by modeling the probability distribution of the data. Transfer learning, in my opinion, is one of the most useful types of techniques for a common project or business. Transfer learning consists of solving a problem and using the knowledge acquired to solve a different problem. These problems can differ on the dataset, for example, training a model to solve a problem with an abundant public dataset and then solving the same problem but with a smaller dataset which belongs to your use case. It can also be the same dataset but two different tasks, and instead of solving the tasks separately the algorithm uses the knowledge from solving one into the other. Transfer learning, in essence, gives a head start to solve a problem. Deep reinforcement learning is a very particular type of technique. It consists of learning a task by trying it many times and learning from failed and successful trials. The approach is similar to how a child learns to drive a bike, the exact instructions are so complex that the only way to learn is to try it. The use cases for deep reinforcement learning require simulators most of the time since it is costly to perform many failed trails. 4 I personally know of only a few actual business cases for deep reinforcement learning, but it is a very hot topic at the moment and certainly state-of-the-art. Meta learning, also known as \"learning to learn\", studies a human's ability to learn new tasks. This is a concept closely related to the holy grail of AI: artificial general intelligence (AGI). Meta learning tries to develop models that achieve intelligence across many different problem domains. Within deep learning there are already many tools available, some of them developed for a specific type of data or a specific use case. The image below illustrates a few, but there are many more and their number is growing. There is the need for deep learning experts that know which tools fit your use case and to know how to tune them to achieve their peak performance. The business cases are currently driven mostly by supervised learning, with transfer learning becoming very important for companies without the resources and technical expertise of the tech giants. Even though the figure below was presented by Andrew Ng in 2016, it still gives a clear picture of where artificial intelligence stands in industry at the moment. Just start It is said that to start a deep learning project you need a huge amount of data...this is not necessarily true! If you have an idea for an AI company or project remember that you don't need to hit a homerun at the start. There is a positive feedback loop embedded in any AI project development. For example, with the use of transfer learning, you can already develop a minimum viable product with a small quantity of data, and as your idea gets adopted and user data flows in, you can make an increasingly better model. A good example of this is Blue River , which started training models using pictures they were taking from their smartphones. Blue River was recently sold for 300 million to John Deer . The competition for AI dominance Finally, I want to point out there is a war out there, with many of the leading countries heavily investing in AI research centers or AI entrepreneurship, boosting the AI industry in their countries but also trying to retain AI and deep learning talent. I personally live in The Netherlands and I'm sometimes disappointed with the country's lack of vision for an AI-based future. I talk to AI startups here with great ideas which do so much with so little resources, and when I go to San Francisco and see the huge difference in investment it is obvious to me who will end on top. I encourage government, venture capitalists, companies and investors from The Netherlands to take a serious view on the AI situation around the globe. Find the complete slides in here. Big Data Expo is a two-day conference in The Netherlands running for several years with an attendance of around 5000 visitors, and best of all it is free! I would say that the conference is business-oriented, with a focus on use cases, inspirational talks and the future for Big Data and AI in business. â†© Find the slides in here â†© Great efforts are currently being made to solve this problem. If you're interested, search for the topic \"distillation of deep learning\" models. â†© Think of self-driving cars, you don't want to crash a car with every failed trial! â†©","tags":"tech","url":"/big-data-expo-2018.html"},{"title":"Senior Data Scientist @ Schiphol Group","text":"","tags":"work","url":"/2018-10-01-data-scientist-schiphol-group.html"},{"title":"Big Data Expo","text":"https://www.bigdata-expo.nl/en/program/deep-learning-engine-ai-revolution DEEP LEARNING, THE ENGINE OF THE AI REVOLUTION We all remember the boom of Internet companies in the late 90s, then in the late 2000s mobile companies took center stage and have been dominating ever since. A new type is taken the spotlight, this is the era of AI companies, and like it has been before there are two options: adapt or fade away. In order to adapt is very important to understand the basic concepts that underpinned Artificial Intelligence and grasp how Deep Learning became the catalyzer of the AI tech revolution. I'll take you through a series of explanations with a historical overview of Deep Learning, and shine some light over question like: what's the difference with classical Machine Learning? What can it do? What can't it do? Why should my business care? I'll give you concrete examples of revolutionary AI that have converge into products in the areas of health care, drug discovery, Fin Tech, medicine, supply chain, marketing, recruiting, customer experience and e-commerce. Finally, I'll communicate my opinion on how the development of an AI feature, AI application or AI business should flow and give my advice on how you create something of value under the shadow of the AI giants like Google, Microsoft, Apple etc.","tags":"talk","url":"/2018-09-12-big-data-expo.html"},{"title":"Code breakfast transfer learning","text":"","tags":"workshop","url":"/2018-06-28-code-breakfast-transfer-learning.html"},{"title":"Spark Summit + AI 2018","text":"This post was originally published in the GoDataDriven blog Last Tuesday and Wednesday Ivo Everts and I attended the Spark+AI Summit 2018 conference in San Francisco. Ivo gave a presentation about Predictive Maintenance at the Dutch Railways and I presented the AI case GDD implemented at Royal FloraHolland Operation Tulip: Using Deep Learning Models to Automate Auction Processes . As a data scientist, I really appreciated that there was a data science track, a deep learning track, and an AI track. Initially, expected to be mostly engineering, but as you will see below, there was plenty of good data science around. Here are the highlights of the talks I attended each day. 1 Note: for those non-technical readers I list some non-tech talks. Day 1 Project Hydrogen: Unifying State-of-the-art AI and Big Data in Apache Spark Reynold Xin (Co-founder and Chief Architect @ Databricks) Databricks unveiled project Hydrogen, which aims to solve the fact that distributed ETL Spark jobs don't play well together with deep learning frameworks. As Databricks Chief Architect Reynold Xin says, there's a fundamental incompatibility between the Spark scheduler and the way distributed machine learning frameworks work. 2 Project Hydrogen introduces gang scheduling which makes possible to have a single framework for ETL data pipelines and deep learning models. In addition, it aims to provide hardware awareness at the task level such that the ETL data pipeline runs in commodity CPU but the deep learning model runs in GPUs for example. Infrastructure for the Complete ML Lifecycle Matei Zaharia (Co-founder and CTO @ Databricks & creator of Spark) This was quite an exciting keynote talk. CTO Matei Zaharia, unveiled and demoed the new open source project mlflow . Mlflow aims to help data scientists track experiments, deploy models and best of all it supports a vast variety of machine learning tools. You can read more in the blog post from Matei this week. The Future of AI and Security Dawn Song (Professor @ UC Berkeley) Professor Dawn Song talked about three vulnerabilities of AI: Attacks to AI models Misuse of AI Data leaks She gave nice examples of the three and demonstrated adversarial attacks in real life like the image and video shows. Your browser does not support the video tag. She then talked about how to resolve some of the open questions and how we can move forward while having these 3 aspects into account. Time Series Forecasting Using Recurrent Neural Network and Vector Autoregressive Model: When and How Jeffrey Yau (Chief Data Scientist @ AllianceBernstein) Jeffrey's talk was of great value for many Data Scientist that deal with time series. He started explaining the difference from univariate vs multivariate analysis in the dynamics of time series, followed by a quick explanation of why is better to use vector autoregressive models instead of ARIMA models. He included two examples and showed how to actually do it. Then he showed how Flint (a time series library for Spark) can be used to preserve the natural order of time-series data when using Spark. He then showed how you can mix Spark and StatsModel time series module to tunned hyperparameters. He then finalized by introducing LSTMS using Keras and making a comparison with a many-to-many model vs VARs models with a prediction of 16 steps ahead. Graph Representation Learning to Prevent Payment Collusion Fraud aud Prevention in Paypal Venkatesh Ramanathan (Data Scientist @ PayPal) This talk was actually pretty cool, the use case was to catch a type of fraud transaction that involves several people, from the seller and buyer side. The talk started by explaining how to map the transactions to a graph-based representation of sellers and buyers. Then he proposed several solutions to detect the fraud, for example, he explained how to use node2vec to find a vector representation for the nodes in the graph and then use those representations in different ML models. He also touched in more advanced algorithms where a temporal component in the graph was introduced and touched into graph convolutions as well. Day 2 ML Meets Economics: New Perspectives and Challenges Michael I. Jordan (Professor @ UC Berkeley) I actually like these type of talks a lot, where AI and especially Deep Learning gets put in a much broader and impactful perspective. Professor Jordan gave very interesting points of how new economic markets can arise from AI if we change our approach to its monetization. He gave a clear example with the music industry among others and provided a list of topics he believes AI practitioners should follow when creating AI systems. He also heavily criticized the current way of AI development. I didn't really agree with several of his points of view but it is always extremely beneficial to hear both sides and rock the boat a little bit. Fireside Chat with Marc Andreessen and Ali Ghodsi Marc Andreessen (Co-founder and partner @ Andreessen Horowitz), Ali Ghodsi (CEO @ Databricks) This was a sitdown where the Ali sort of interviews Marc (an influential venture capitalist). It's the perfect talk to hear during getting ready in the morning or on your way to work. They touched a bit on the history of tech companies and how company pitches have evolved with the rising of AI. Also, Marc gave some helpful pointers to startups on what is a venture capitalist looking for. A particular discussion that stuck was if AI is a truly revolutionary technology or just an add-on feature? Building the Software 2.0 Stack Andrej Karpathy (Director of AI @ Tesla) I really liked this one, Andrej encapsulated in a concept what we all have experienced after productionazing several machine learning models. He talked about Software 2.0, this concept basically tells us that the programming in AI is now being done by labelers. What we as data scientist do is just choose a big chunk of the solution space and the data then finds the best program for our case in that space using the data. Given that belief, he mentioned how in Tesla he has been spending most of this time making sure that the dataset labels are of very high quality. He gave some funny examples of extremely rare data he has come across and reiterated the importance of having a robust and quality labeling system. I really liked how genuine his comments were and to see that even at Tesla they have these sort of \"mortal\" issues that I also face. Deep Learning for recommender systems Nick Pentreath (Principal Engineer @ IBM) Together with the time-series talk the most beneficial for a data scientist. The talk from Nick was very well structured and explained. He started from the basic methods like item-item and matrix factorization which are based on feature and explicit interactions or events. He then set the landscape of the current most common case which involves explicit, implicit, social and intent events. He then addressed the cold start problem and explained why the standard/old collaborative filtering models break down with the current need of applications. Then deep learning approaches were covered and explained how implicit events can be used in the loss function of a neural network. Then he followed by showing the state-of-the-art deep learning implementations like DeepFM . 3 The next part was even sort of new to me, he added session-based recommendations plus the content discussed before by using recurrent neural networks on top of the networks before. I was happily relieved that I was up to date with most of the state-of-the-art deep learning applications to recommendation systems, but also learned something new after a chat with Nick and a data scientist from Nike dealing with the same problems. Nandeska? Say What? Learning, Visualizing, and Understanding Multilingual Word Embeddings Ali Zaidi (Data Scientist @ Microsoft) This talked discussed how to find similar embedding spaces for words with the same meaning regardless of the language, pretty cool stuff. Ali started by noticing that big datasets for domain-specific NLP are quite scarce, so he shared some of them. Ali then showcased how to learn Word2Vec embeddings at scale with Spark via the Azure text analytics package (tatk). Then he moved on to explain how by calculating the embeddings individually for each language and then throwing them into a domain adaptation using an adversarial objective you can achieve the desired objective. He showed this for Spanish and Russian. I don't know how well did the algorithm worked for most of the words in general but the approach and little sample result shown was quite nice. AdiÃ³s In conclusion I was happy with the content of the conference and will recommended to data scientist to take a look next year, also the network I made during discussing is priceless. 4 It was also fun to talk at the conference, I left with a good feeling and was able to deliver some good jokes ðŸ˜‰. But of to be honest the best of everything was the place we found with authentic Mexican food! I almost cried of the excitement... they even had agua de horchata ðŸ˜‚ As always I'm happy to discuss and answer any further questions about the conference or other things, just ping me on twitter @rragundez or LinkedIn. The slides and presentations haven't been uploaded yet. I'll update the links once they are released. â†© Spark divides jobs into independent tasks (embarrassingly parallel), this differs from how distributed machine learning frameworks work, which sometimes use MPI or custom RPCs for doing communication. â†© I would recommend you to first start with the LightFM implementation described here . â†© As a side note, in the talks of the big companies I saw a LOT of tensorflow inside Spark. It made me wish that NL companies would have that much volume of data. â†©","tags":"tech","url":"/spark-summit-ai-2018.html"},{"title":"Spark + AI Summit","text":"https://databricks.com/session/operation-tulip-using-deep-learning-models-to-automate-auction-processes Operation Tulip: Using Deep Learning Models to Automate Auction Processes We are using Deep Learning models to help Royal Flora Holland automate their auction processes. With over 100,000 transactions per day and 400,000 different types of flowers and plants, Royal Flora Holland is the biggest horticulture marketplace and knowledge center in the world. An essential part of their process is having the correct photographs of the flower or plants uploaded by suppliers. These photos are uploaded daily and could have requirements. For example, some images require a ruler to be visible or a tray to be present. Manual inspection is practically impossible. Using Keras with a Tensorflow backend we implemented a Deep Neural Network (DNN) using transfer learning for each screening criteria. We also apply heuristics and business rules. The goal is to give real-time feedback at upload time, this challenged us to run multiple deep learning models in real-enough-time. During the journey of building the Image Detection system we have used specific implementations that can be insightful and helpful to the audience. For example, our models are not only trained in parallel but transfer learning allows us to engineer a single 1st component for all models and then having the flow distribute over each of the DNN (~90% of the work is shared among the DNNs). Our models achieve above 95% accuracy and because of the component-like architecture it's very flexible. Session hashtag: #AISAIS11","tags":"talk","url":"/2018-06-06-spark-ai-summit.html"},{"title":"Unsupervised and Reinforcement Learning","text":"","tags":"study","url":"/2018-06-02-unsupervised-and-reinforcement-learning.html"},{"title":"Dutch Data Science Week","text":"https://www.eventbrite.nl/e/tickets-training-special-deep-learning-dutch-data-science-week-2018-44832464107# Deep learning Special with Python, Tensorflow and Keras with a focus on Recurrent Neural Networks and LSTMs. Every theory part is complemented by a hands-on session, the goal is that you become familiar with the theory but also learn the how to apply the theory in practice with several exercises. Curriculum Deep Learning basics (Theory) Keras API with image classification (Hands-on) Neural networks in practice (Theory) Predicting bank term deposits (Hands-on) Recurrent Neural Networks (Theory) Forecasting airline passengers with RNNs (Hands-on) Long short-term memory (Theory) Human activity recognition with LSTMs (Hands-on) NLP sentiment classification with LSTMs (Hands-on) Introduction to Gated recurrent units (Theory) Q&A Some of the things you will learn are: The Keras API Pragmatic best practices when using Deep Learning models Recognize cases when Recurrent Neural Networks are useful Pre-process time-series data for an RNN or LSTM Combine several time-series for a single RNN or LSTM model Use many-to-one RNN and LSTM models Use many-to-many RNN and LSTM models Process text data for an RNN or LSTM model Prerequisites Experience in Python is advised for the hands-on sessions Experience with Machine Learning concepts (e.g. regularization, overfitting, feature scaling, hyperparameter optimization) Basic familiarity with Deep Learning Activities The course is dynamic with ideas exchanging and open communication. There are also some fun activities based on the course content. Instructor The workshop will be given by Rodrigo Agundez. Data Maverick at GoDataDriven. Rodrigo has been giving training sessions and workshops for several years now, he gave a Deep Learning Tensorflow workshop during the Data Science Summit Europe 2016 in Israel, is one of the current trainers for the Data Science with Python, the time-series lecture and Deep Learning training (GoDataDriven). In addition, as a consultant, he has seen many use cases and can help you with specific questions that relate to using Data Science in practice, productizing models, etc. TAGS","tags":"workshop","url":"/2018-05-29-dutch-data-science-week.html"},{"title":"PyData Amsterdam","text":"https://pydata.org/amsterdam2018/schedule/presentation/30/ Hands-on introduction to Deep Learning with Keras and Tensorflow Audience level: Novice Description Deep Learning has already conquered areas such as image recognition, NLP, voice recognition, and is a must-know tool for every Data Practitioner. This tutorial for aspiring Deep Learners will consist of a quick blunt Deep Learning overview followed by a hands-on tutorial that will teach you how to get started using Keras and Tesorflow. Abstract Deep Learning has already conquered areas such as image recognition, NLP, voice recognition, and is a must-know tool for every Data Practitioner. This tutorial for aspiring Deep Learners will consist of a quick blunt Deep Learning overview followed by a hands-on tutorial that will teach you how to get started using Keras and Tesorflow. This tutorial is for people that know the fundamentals of machine learning have a worked with the PyData stack have no deep learning hands-on experience with Keras Curriculum Deep Learning landscape Deep Learning tools in Python Blunt review of the Keras API (Hands-on) Build a deep learning model for an easy image classification dataset (Hands-on) Play around and optimize deep learning model for a harder dataset (Hands-on) Prerequisites Experience with Python and jupyter notebooks Keras or Tensroflow (version >= 1.4) installed Note: Some of the material is a repeat of the Code Breakfast Deep Learning session of January 17, 2018","tags":"workshop","url":"/2018-05-26-pydata-amsterdam.html"},{"title":"Deep learning to Deloitte","text":"","tags":"workshop","url":"/2018-05-20-deep-learning-deloitte.html"},{"title":"Elitist shuffle for recommendation systems","text":"This post was originally published in the GoDataDriven blog In today's high pace user experience it is expected that new recommended items appear every time the user opens the application, but what to do if your recommendation system runs every hour or every day? I give a solution that you can plug & play without having to re-engineer your recommendation system. The common practice to update recommended items is to have the recommendation system re-score the available items every period of time T . This means that for a whole period T , the end-user faces the same content in the application's entry screen. In today's high pace user experience if T is even a few hours, let alone a day, the user can get bored of the same content displayed every time it opens the application during the period T . There can be many ways this scenario can happen but imagine the user opens the application and doesn't like the recommended items and is too lazy or busy to scroll or search for something else. If the user opens the application again some minutes later to find exactly the same content as before this might have a big (negative) impact on the retention for this user. An obvious solution to this problem is to shuffle the content in such a way that it remains relevant to the user while new content appears on the screen each time the user re-opens the application. Below there are two screen shots from my YouTube account a couple of seconds apart with no interaction, just clicking the refresh button. We can notice several things: Content is still relevant. Content is not the same. Some content has changed position. Some new content has appeared. This can be because YouTube re-scores items in a very short time T or runs an online algorithm. 1 What can you do to achieve something similar if your recommendation system has a T in the order of hours? In this blog post, I propose a simple solution based on a non-uniform shuffling algorithm that you can basically plug & play or build on top off. Example scenario Suppose you have 10,000 items in total that can be recommended to your user, you run the recommendation system over all the items and those 10,000 items get ranked in order of relevance of the content. 2 The application shows 5 items on the entry screen. The first time the user opens the application after the re-scoring process the top 5 ranked items are shown. It is decided that from now on (based on user control groups, investigation, AB testing, etc.) until the next re-scoring process the entry screen should not be the same every time and remain relevant for the user. Based on an investigation from the data scientist it turns out that somewhat relevant items appear until item 100. 3 Then the idea is to somehow shuffle those 100 items such that the top 5 items shown are still relevant but not the same. In order for the figures of this blog post to be more readable and understandable, I'll use a hypothetical threshold of 20 items and not 100. Fisherâ€“Yates shuffle / uniform Shuffling in Python is a very common action and can be done using the random module which contains the shuffle function . >>> print ( inspect . getsource ( random . shuffle )) def shuffle ( self , x , random = None ): \"\"\"Shuffle list x in place, and return None. Optional argument random is a 0-argument function returning a random float in [0.0, 1.0); if it is the default None, the standard random.random will be used. \"\"\" if random is None : randbelow = self . _randbelow for i in reversed ( range ( 1 , len ( x ))): # pick an element in x[:i+1] with which to exchange x[i] j = randbelow ( i + 1 ) x [ i ], x [ j ] = x [ j ], x [ i ] else : _int = int for i in reversed ( range ( 1 , len ( x ))): # pick an element in x[:i+1] with which to exchange x[i] j = _int ( random () * ( i + 1 )) x [ i ], x [ j ] = x [ j ], x [ i ] This shuffle method uses the optimized Fisherâ€“Yates algorithm introduced by Richard Durstenfield in 1964 which reduced the running time from \\(O(n&#94;2)\\) to \\(O(n)\\) . By default the algorithm produces a uniform shuffle of an array in which every permutation is equally likely. This means that an item has equal probability to end up in any position. 4 Below you can find an animation of the results of the random.shuffle default algorithm. I show the initial position of an item in red and the expected probability distribution of landing in any position after 5000 shuffling simulations. This type of shuffle is not beneficial for our purpose as there is the same probability of the least recommended item to appear on top than any other, this is definitely not the way to go since we can end up with very poor recommendations on top. Fisherâ€“Yates shuffle / non-uniform Notice that the shuffle function shown above has the parameter random which is described in the docstring as follows: def shuffle ( self , x , random = None ): \"\"\"Shuffle list x in place, and return None. Optional argument random is a 0-argument function returning a random float in [0.0, 1.0); if it is the default None, the standard random.random will be used. \"\"\" If you try to understand the Fisher-Yates algorithm and then look at the source code, you notice that the random parameter affects the location where intermediate swaps will happen and that the effect of a non-uniform random distribution parameter is quite difficult to predict. It kept my mind busy for some hours. I tried different functions to pass to the random parameter but they all behaved strange and unexpected in one way or another, for example let's try a \\(\\beta\\) distribution such that the first draws are very likely to be swapped with elements at the end (higher probability near 1.0). 5 The simulation below uses the \\(\\beta\\) -distribution as the random parameter. This approach does allocate higher probabilities towards higher positions for higher initially ranked items, but the distribution is highly non-symmetrical and very different for different initial positions. I find it surprising that at some point the initial position does not have the maximum probability. 6 Also, I find it very hard to explain the relation between the given \\(\\beta\\) -distribution and the resulting probability distribution . I played with the parameters and other distributions but still noticed strange behavior. This will make it quite difficult to explain the expected impact on the recommended items to the user. Elitist shuffle This is actually a simple approach, I shuffle the items by choosing items with a weighted probability (this is the same as sampling from a multinomial distribution without replacement). I won't go into the details but the function numpy.random.choice with the parameter replace=False does what we want, it is just a matter of choosing the appropriate weight probabilities. In this case I choose to set the weights by transforming the reverse position as np.linspace(1, 0, num=len(items), endpoint=False) . 7 Then I introduce a parameter called inequality as a knob to tune the weight probability difference between positions. >>> print ( inspect . getsource ( elitist_shuffle )) def elitist_shuffle ( items , inequality ): \"\"\"Shuffle array with bias over initial ranks A higher ranked content has a higher probability to end up higher ranked after the shuffle than an initially lower ranked one. Args: items (numpy.array): Items to be shuffled inequality (int/float): how biased you want the shuffle to be. A higher value will yield a lower probabilty of a higher initially ranked item to end up in a lower ranked position in the sequence. \"\"\" weights = np . power ( np . linspace ( 1 , 0 , num = len ( items ), endpoint = False ), inequality ) weights = weights / np . linalg . norm ( weights , ord = 1 ) return np . random . choice ( items , size = len ( items ), replace = False , p = weights ) As the simulation below shows, this approach gives a clearer picture of what's going on and it let us tune the algorithm using the inequality parameter according to the requirements of our application. This is an animation based on 5000 simulations with inequality=10 From the animation we notice: The maximum probability remains on the initial position. Probability decays monotonically with the distance from the initial position. The distribution is non-symmetrical but smoother than the previous example. Higher ranked items have a higher chance of being moved from their initial position. A big win is that the inequality parameter has a direct understandable impact on the resulting distributions, want higher items to be more probable to remain on top? Increase inequality. In addition, the behavior translates into the desired functionality: Top content would still be relevant after shuffle. Content is not the same. Some content has changed position. Some new content has appeared. Drawback The elitist_shuffle function is much slower than np.random.shuffle , but still fast for a common application. Coming back to the example scenario where the items to shuffle are 100 , the elitist_shuffle function takes around 1.8ms . If this is too slow for you I would recommend to first try numba with the no_python parameter enabled and then if necessary try a Cython implementation. AdiÃ³s As final remarks, I advise you to: First, discuss with your team if you need a feature like this. There are applications where the user might be expecting to find the same items it saw last time. Perhaps trigger this behavior if more than x seconds have passed. Add the recommendation system scores to the calculation of the weight probabilities. This could just be setting the weights to the scores before the exponentiation and \\(l&#94;1\\) normalization ðŸ˜‰. As always I'm happy to discuss and answer any questions, just ping me on twitter @rragundez . You can find the code here . Some other user similar to me might have done some actions that affect my recommendations, or simply not clicking on the items affects my own recommendations. â†© There can be an exploration-exploitation step after. â†© It can also be a dynamic threshold based on the scores from the recommendation system. â†© This algorithm is also used by numpy . â†© This is what we want since the algorithm first swaps elements from the end (look at reversed in line 303 . â†© It is not a matter of increasing the number of simulations. I did that and found the same behavior. â†© You might be tempted to use np.arange(len(items), 0, step=-1) which is not numerically robust for a big inequality parameter. â†© if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"tech","url":"/elitist-shuffle.html"},{"title":"How Deep Learning Will Change Customer Experience","text":"I was co-author to this article and was originally published by Ronald van Loon Deep learning is a sub-category within machine learning and artificial intelligence. It is inspired by and based on the model of the human brain to create artificial neural networks for machines. Deep learning will allow machines and devices to function in some ways as humans do. Dr. Rodrigo Agundez of GoDataDriven is co-author of this article and very enthusiastic about the improvements that deep learning can offer. He's been involved in the data science and analysis field for some time, and is already working on implementing models for practical applications. Rodrigo notes that the new generation of users wants to interact with devices and appliances in a human-like manner. Take the example of Apple's Siri, which allows for voice command and voice recognition. Communicating with Siri is similar to interacting with a human. The user interface for Siri seems simple enough. However, the A.I. algorithms that are designed on the back-end are quite complex. Designing this kind of interaction with a machine was not possible a few years ago. System designers now have access to complex deep learning algorithms that makes it possible to integrate such behavior into machines. Importance of Deep Learning Artificial Intelligence will never truly come of age without giving machines the powerful capabilities of deep learning. The idea of designing deep learning models can be difficult to grasp for many people. This is because understanding human concepts comes naturally to us. But giving the same ability to machines is a very complex process of design. One way to do it is by structuring data in a way that makes it easier to process for machines. Take the word \"fat\" for instance. If we say to a friend, \"This burger has too much fat,\" they would understand what we mean and the word would have a negative connotation here. But if we told a friend that \"I would love to get fat from this meal any day,\" the word would mean something entirely different. Creating machines that are capable of understanding minute differences in words embedded in a context may seem like a very small thing, but requires a very large set of data and complex algorithms to execute. Difference from Traditional Machine Learning One way to differentiate between traditional machine learning and deep learning is through the use of features. These are the characteristics of the data that help us differentiate and identify one entity from another. To understand features better, take the example of a normal bank transaction. Features of the transaction help us identify the timing of the transaction, the value transferred, names of the parties to the transaction, and other important information. In a traditional machine learning model, features have to be designed by humans. In a deep learning model, features are identified by the A.I. itself. We can take another example of differences between a cat and a dog. If we showed a person a cat and a dog and asked them to point to the cat, they would immediately identify it. However, if the same person was asked to identify the exact features that differentiate the two, they would have a problem. Both creatures have four legs, a body, a tail, and a head. They appear very similar in terms of features. Humans can distinguish one from the other in an instant. Yet, they would have trouble identifying the features that differentiate any pair of a cat and a dog. This is a problem that data scientists and A.I. developers hope to solve with deep learning. Features can be found even in unstructured data with the help of deep learning algorithms. Benefit from Deep Learning for the Customer Experience Rodrigo states that deep learning models are superior at certain A.I. characteristics than any traditional machine learning models, as the models has shown its effectiveness. This can be traced back to 2012 where in a known online image recognition challenge, a deep learning algorithm proved to be twice as effective as any other algorithm before. If an A.I. model reaches an accuracy of 50%, the device would not be very practical for use. Take the example of automobiles. A person would not trust getting in a car where brakes work 50% of the time. However, if the accuracy of an A.I. system reaches values around 95%, it would be much more reliable and robust for practical use. Rodrigo believes that this level of accuracy for human-like tasks can only be achieved with deep learning algorithms. Deep learning can be applied to speech recognition to improve customer experience. Speech recognition technology has been around for quite some time, but it didn't cross the accuracy boundary to become a marketable product until the introduction of deep learning models. Home automation systems and devices work through voice command. This is an area where deep learning can significantly improve customer experience. Royal FloraHolland Case Royal FloraHolland is the biggest horticulture marketplace and knowledge center in the world. An essential part of their process is having the correct photographs of the flower or plants uploaded by suppliers. These photos need to have a plant, some images require a ruler to be visible or a tray to be present. The task of sorting through all these photographs manually and quickly is basically impossible, therefore it was decided to implement A.I. for the process. GoDataDriven designed a system with deep learning algorithms to automate the checking of the images. The system can accurately identify and sort pictures taken from different angles and devices. The system removed the need for manual human review and completely automated the process for the company. University Medical Centrum Groningen (UMCG) Deep learning algorithms were developed for UMCG with collaboration from GoDataDriven, Google and Siemens. This involved the use of MRI data in a 4D format (volume + time). Using deep learning models, the team calculated the heart ventricles volumes evolution over time. One of the project goals is to assist in the decision making regarding pace makers and treatments. For example, it could take the heart cycle and volumes into consideration for prognosis and heart failure. More than 400 images were taken per patient for different hearth depths across time. The team at GoDataDriven and Siemens developed multiple models, including binary and multi-class segmentation. The model based on the U-Net deep learning architecture takes the MRI scan as input and outputs the corresponding volumes. Traditionally, the process is done manually by looking at the scans and interpreting the results through hand-drawn diagrams. Future of Deep Learning Deep learning provides a way for companies to develop life-long learning modules. When more complex and richer algorithms are developed on top of pre-existing ones, companies will be able to achieve incremental growth. Rodrigo believes that deep learning has a bright future because of its open source community and accessible platforms. Major corporations such as Apple which had built their systems on secrecy are finally coming around to the open-source model. The main reason they are switching now is because they find deep learning talent acquisition more difficult in comparison with open source companies, such as Google's Deep Mind for example. A company could have developed the most amazing and efficient deep learning system but if they don't publish their research and share the knowledge online, talented data scientists and deep learning practitioners will not apply to this companies. Currently deep learning teams like Google Brain, Google Deep Mind and companies like Facebook and Baidu find it much easier to hire talented deep learning practitioners. They continuously publish research and open source the related implementations, such that the deep learning is reminded that these companies are at the cutting edge of these technologies. Since the shift is towards open source and global adaptation of this technology, deep learning is likely to do well in the future and impact vast sectors of in our society. To learn more about Deep Learning and join the Dutch Data Science week click here.","tags":"tech","url":"/dl-customer-experience.html"},{"title":"Python masterclass to Restart Network","text":"","tags":"workshop","url":"/2018-05-02-python-masterclass-restart-network.html"},{"title":"Advanced deep learning to bol.com, Delhaize and AHOL group","text":"","tags":"workshop","url":"/2018-04-01-advanced-deep-learning-ahol.html"},{"title":"Multi-threshold Neuron Model","text":"This post was originally published in the GoDataDriven blog Inspired by a new biological scientific research, I propose, build and train a Deep Neural Network using a novel neuron model. Figure 0. Schematic diagrams of two neuron models. (Central-threshold neuron) The model on the left is the current employed model in artificial neural networks where the input signals are propagated if their sum is above a certain threshold. (Multi-threshold neuron) In contrast, in the right I show the new proposed model where each input signal goes through a threshold filter before summing them. In this blog post I construct and train a simple Deep Neural Network based on a novel experimental driven neuron model proposed last year (2017) in July. This blog is separated as follows: Scientific background Summarize the article that lead me to this idea and explain some of the theory. Concepts Relate Deep Learning technical concepts to Neuroscience concepts mentioned in the paper. Approximations Introduce approximations I will make on the multi-threshold neuron model. Discussion Overview of mathematical and Deep Learning implications as a consequence of the multi-threshold neuron model. Model & training Tensorflow implementation and training of a simple fully-connected Deep Neural Network using the multi-threshold neuron model. Results Briefly show and explain results from training and testing the proposed model vs the commonly used one in Deep Neural Networks (DNNs). Scientific background S. Sardi et al. published in July last year (2017) an experimental work in Nature scientific reports which contradicts a century old assumption about how neurons work. The work was a combined effort between the Physics, Life Sciences and Neuroscience departments of Bar-Ilan University in Tel Aviv, Israel. The authors proposed three different neuron models which they put to the test with different types of experiments. They describe each neuron model with, what they call, neuronal equations . Figure 1. Schematic representation of a neuron. The signal in a neural network flows from a neuron's axon to the dendrites of another one. That is, the signal in any neuron is incoming from its dendrites and outgoing to its axon. Below I describe two of these neuron models in the paper. In particular the commonly used neuron model which I call \"central-threshold\" and the neuron model proposal in this blog \"multi-threshold\". Central-threshold neuron This is the current adopted computational description of neurons ( artificial neurons ), and the corner stone of Deep Learning. \"A neuron consists of a unique centralized excitable mechanism\". The signal reaching the neuron consists of a linear sum of the incoming signals from all the dendrites connected to the neuron, if this sum reaches a threshold, a spike signal is propagated through the axon to the other connected neurons. The neuronal equation of this model is: $$I = \\Theta\\Big(\\sum_{i=1}&#94;NW_i\\cdot I_i - t\\Big)$$ where \\(i\\) : identifies any connected neuron \\(N\\) : total number of connected neurons \\(W_i\\) : is the weight (strength) associated to the connection with neuron \\(i\\) \\(I_i\\) : is the signal coming out of neuron \\(i\\) \\(t\\) : is the centralized single neuron threshold \\(\\Theta\\) : is the Heaviside step function \\(I\\) : signal output from the neuron Multi-threshold neuron In this model the centralized threshold ( \\(\\Theta\\) ) is removed. The neuron can be independently excited by any signal coming from a dendrite given that this signal is above a threshold. This model describes a multi-threshold neuron and the mathematical representation can be written as: $$I=\\sum_{i=1}&#94;N\\Theta(W_i\\cdot I_i - t_i)$$ where \\(i\\) : identifies any connected neuron \\(N\\) : total number of connected neurons \\(W_i\\) : is the weight (strength) associated to the connection with neuron \\(i\\) \\(I_i\\) : is the signal coming out of neuron \\(i\\) \\(t_i\\) : is the threshold value for each neuron \\(i\\) \\(\\Theta\\) : is the Heaviside step function \\(I\\) : signal output from the neuron Study conclusion Based on their experiments the authors conclude that the multi-threshold neuron model explains best the data. The authors mention that the main reason for adopting the central-threshold neuron as the main model, is that technology did not allow for direct excitation of single neurons, which other model experiments require. Moreover, they state that these results could have been discovered using technology that existed since the 1980s. Concepts There are some main concepts in the Deep Learning domain that you should be familiar with before proceeding. If you are familiar with them skip this part. Artificial neuron A mathematical representation of a biological neuron. They are the corner stone of artificial neural networks and Deep Learning. The idea is that the artificial neuron receives input signals from other connected artificial neurons and via a non-linear transmission function emits a signal itself. Activation function The current understanding of a neuron is that it will transmit some signal only if the sum from incoming signals from other neurons exceeds a threshold. For an artificial neuron this threshold filter is applied via an activation function. There are many activation functions but the Rectified Linear unit (ReLu) is one of the most broadly used in the Deep Learning community, and it's the one I will use in this notebook. The mathematical definition of the function is: $$R(z) = max(0, z) = \\begin{cases} 0 &\\quad\\text{for } z\\leq0 \\\\ z &\\quad\\text{for } z > 0 \\end{cases}$$ You can check its implementation in the Tensorflow source code or in the Tensorflow playground code . Approximations I am a theoretical physicist and as such it's impossible for me to resist the spherical cow . Single threshold value The multi-threshold neuron model contains different threshold parameter values ( \\(t_i\\) ). Mathematically a threshold has the same effect if I take it as a constant and instead the input signal is moved up or down by the connecting weight parameters. Hence, the neuronal equation becomes: 1 $$I=\\sum_{i=1}&#94;N\\Theta(W_i\\cdot I_i - t)$$ ReLu activation function I'll replace the Heaviside step function ( \\(\\Theta\\) ) with threshold \\(t\\) by a Rectified Linear unit ( \\(\\mathcal{R}\\) ). $$I=\\sum_{i=1}&#94;N\\mathcal{R}(W_i\\cdot I_i)$$ In general any activation function could replace the Heaviside step function. Bias Notice that the proposed model equation contains no bias terms. I'll add a bias term to the equation since it's known to help neural networks fit better. It can also help with the threshold approximation, tuning the biases instead of the thresholds. $$I=\\sum_{i=1}&#94;N\\mathcal{R}(W_i\\cdot I_i) + b$$ Discussion The idea is to take the multi-threshold neuron model and try to write a Deep Learning implementation, a neural network consisting of multi-threshold neurons. Tensorflow is quite flexible and allows for writing user defined implementations. Backpropagation In order for my neural network to be trained I need backpropagation, this means that the derivative of whatever I introduce is necessary. Luckily, I'm not changing the activation function itself, I can just use the already derivative of the ReLu function in Tensorflow: $$\\frac{d}{dz}\\mathcal{R}(z)= \\begin{cases} 0 &\\quad\\text{for } z\\leq0 \\\\ 1 &\\quad\\text{for } z > 0 \\end{cases}$$ You can check it out in the Tensorflow source code or in the Tensorflow playground code . Tensor multiplication What I'm really changing is the architecture of the artificial neural network as seen in Figure 0, the activation function is no longer applied on the sum of all the inputs from the connected neurons, but instead on the input arriving from every single connected neuron. The sum operation is going from inside the activation function to outside of it: $$\\mathcal{R}\\Big(\\sum_{i=1}&#94;NW_i\\cdot I_i\\Big) \\rightarrow \\sum_{i=1}&#94;N\\mathcal{R}(W_i\\cdot I_i)$$ Do you see the implementation problem described by the equation above? In the central-threshold model (left equation) the input to the activation function \\(\\sum_iW_i\\cdot I_i\\) is exactly the dot product between vectors \\((W_1, W_2,\\dots,W_N)\\) and \\((I_1, I_2,\\dots,I_N)\\) and it's this fact which allows fast computation of input signals for many neurons and observations at once via a single matrix multiplication. In the multi-threshold model this is no longer possible. I think this will be the biggest challenge when coming up with an implementation which can be trained efficiently and fast. Example Suppose I have the following weight matrix connecting two neuron layers, the first layer has 3 neurons the second has 2: $$W= \\begin{bmatrix} 3 & -4 \\\\ -2& 2\\\\ 0& 4 \\end{bmatrix} $$ and that the output signal from the neurons in the first layer are $$I_0= \\begin{bmatrix} 2 & 5 & 1 \\end{bmatrix} $$ with bias terms $$b= \\begin{bmatrix} 2 & -1 \\end{bmatrix} $$ Using the standard central-threshold neuron model, the output signal of the second layer is: $$\\mathcal{R}\\Big(I_0\\cdot W + b\\Big) = \\mathcal{R}\\Big( \\begin{bmatrix} 2 & 5 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & -4 \\\\ -2& 2\\\\ 0& 4 \\end{bmatrix} + \\begin{bmatrix} 2 & -1 \\end{bmatrix} \\Big) = $$ $$ \\mathcal{R}\\Big( \\begin{bmatrix} -2 & 5 \\end{bmatrix} \\Big) = \\begin{bmatrix} \\mathcal{R}(-2)& \\mathcal{R}(5) \\end{bmatrix} \\Big) = \\begin{bmatrix} 0 & 5 \\end{bmatrix} $$ In the case of the multi-threshold neuron model proposed the output is $$ [\\sum_{i=1}&#94;N\\mathcal{R}(W_{i1}\\cdot I_i) + b_1, \\sum_{i=1}&#94;N\\mathcal{R}(W_{i2}\\cdot I_i) + b_2]= $$ $$ \\begin{bmatrix} \\mathcal{R}(6) + \\mathcal{R}(-10) + \\mathcal{R}(0) + 2 & \\mathcal{R}(-8) + \\mathcal{R}(10) + \\mathcal{R}(4) -1 \\end{bmatrix} = \\begin{bmatrix} 8 & 13 \\end{bmatrix} $$ As the example shows, a fundamental difference is that in the multi-threshold case if any input output signal times the weight is positive then the output will be positive. This will greatly reduce the sparsity of the neurons firing throughout the network in comparison with the conventional central-threshold model. I don't know all the implications but I expect that it will be more difficult for individual neurons (or parts of the network) to singly address a specific feature, therefore in principle reducing overfitting. A known issue of most activation functions in Deep Neural Networks is the \"vanishing gradient problem\", it relates to the decreasing update value to the weights as the errors propagate through the network via backpropagation. In the standard central-threshold model the ReLu partially solves this problem by having a derivative equal to 1 if the neuron fires, this propagates the error without vanishing the gradient. On the other hand, if the neuron signal is negative and squashed by the ReLu (did not fire) the corresponding weights are not updated, since the ReLu derivate is zero i.e. neuron connections are not learning when the connecting neurons didn't fire. In the multi-threshold model, I expect this last issue to be reduced since sparsity reduces, more weights should be updated on each step in comparison with the central-threshold neuron. Model & training I first concentrate in replicating the example above using tensorflow , it contains two built-in related ReLu functions: relu_layer relu The relu_layer function already assumes a layer architecture with central-threshold neurons. The relu function on the other hand can operate on each entry of a tensor. import tensorflow as tf sess = tf . Session () b = tf . constant ([ 2 , - 1 ]) w = tf . constant ([[ 3 , - 2 , 0 ], [ - 4 , 2 , 4 ]]) I_0 = tf . constant ([ 2 , 5 , 1 ]) I_1 = tf . reduce_sum ( tf . nn . relu ( tf . multiply ( I_0 , w )), axis = 1 ) + b I_1 . eval ( session = sess ) >>> array ([ 8 , 13 ], dtype = int32 ) Notice that b and I_0 are one dimensional tensors, this allows me to take advantage of the tensorflow broadcasting feature. Using the code above I can then define a neural network layer consisting of multi-threshold neurons 2 . def multi_threshold_neuron_layer ( input_values , weights , b , activation = tf . nn . relu ): return tf . reduce_sum ( activation ( tf . multiply ( input_values , weights )), axis = 1 ) + b MNIST - 2 hidden layer multi-threshold neural network With this basic implementation, my goal was to see if the model is actually trainable. I just wanted to observe the loss decrease with each iteration. As you probably noticed, the multi_threshold_neuron_layer can only take 1 example at a time, this is the complication I mentioned, simple matrix multiplication taking several observations is no longer possible for now. In part II of the blog I hope to expand to a more efficient implementation. The multi-threshold neural network is then: # Construct model I_0 = tf . placeholder ( \"float\" , shape = ( input_size ,)) # input layer W_01 = tf . Variable ( tf . random_normal (( hidden_layers_sizes [ 0 ], input_size ))) b_1 = tf . Variable ( tf . random_normal (( hidden_layers_sizes [ 0 ],))) I_1 = multi_threshold_neuron_layer ( I_0 , W_01 , b_1 ) # 1st hidden layer W_12 = tf . Variable ( tf . random_normal (( hidden_layers_sizes [ 1 ], hidden_layers_sizes [ 0 ]))) b_2 = tf . Variable ( tf . random_normal (( hidden_layers_sizes [ 1 ],))) I_2 = multi_threshold_neuron_layer ( I_1 , W_12 , b_2 ) # 2nd hidden layer W_23 = tf . Variable ( tf . random_normal (( number_of_classes , hidden_layers_sizes [ 1 ]))) b_3 = tf . Variable ( tf . random_normal (( number_of_classes ,))) output = tf . transpose ( tf . matmul ( W_23 , tf . reshape ( I_2 , shape = ( - 1 , 1 )))) + b_3 # output layer # truth target = tf . placeholder ( \"float\" , shape = ( 1 , number_of_classes )) Using the digits MNIST data set I ran a comparison between a DNN using the conventional central-threshold neurons and the proposed multi-threshold neurons. 3 from tensorflow.examples.tutorials.mnist import input_data mnist = input_data . read_data_sets ( \"/tmp\" , one_hot = True ) Results It is trainable! I actually though this would just crash and burn so I was very happy to see that loss go down :). I calculated the cross-entropy loss and accuracy during training and final accuracy in a test set. It is very important to remember that to keep things fair the calculations for both models are using a batch of 1 observation. The training period ran for 4 epochs with a training set of 55000 observations . Normally the loss and accuracy is calculated over the batch, in this case that makes no sense 4 . Instead what I do is report the average loss and average accuracy over every 1100 observations 5 . The score of my model consisted of calculating the accuracy over a test set of 10000 observations . Training loss and accuracy There are many things that can be discussed from Figure 2 but here are the main points: Cross-entropy loss decreases with iterations which means the model is trainable. When the central-threshold model is performing well its loss is much lower than the multi-threshold . Notice that this is the case since the beginning of the training period, a sort of a shift. This could be because our images contain consistent white areas (edges) where the cross-entropy benefits from having sparse activations in our neural network. The multi-threshold model seems to be more robust against higher learning rates. Moreover, it seems to prefer higher learning rates. As I mentioned before I would expect the multi-threshold model to have less sparse activations which in turn should result in a faster learning 6 . This can be observed for learning rate .0005 and .001 . Test Accuracy Learning rate Central-threshold Multi-threshold 0.0005 0.8571 0.8757 0.001 0.8958 0.8879 0.005 0.2554 0.9085 0.01 0.1028 0.773 As seen in the training report above, the multi-threshold model seems to be more robust against higher learning rates. It could be that this is just a sort of shift and for even bigger learning rates it will show the same behavior as the central-threshold . The multi-threshold model does not overfit in these examples. Even more, for learning rate 0.005 it achieves a loss 2 orders of magnitude higher than the central-threshold but a higher accuracy in the test set. Adios This was a pretty fun blog to make. I have some final remarks: The proposed model is trainable, but I cannot say much of the specifics since that requires more investigation that I have not done. A very important point is that since at the moment I can only use batches of 1, the training time is painfully slow, definitely not something for realistic applications. Finally, I know that Figure 3 seems quite promising but let's not forget that this is done with a batch of a single observation. In part II of this blog I'll try to come up with the functionality of having more observations per update and use a convolutional layer to make a more realistic comparison. You can find the code here . If you have any other questions just ping me in twitter @rragundez . This is also happening in the current neural network implementations, since in reality there is no reason for different neurons to have the same threshold, nevertheless commonly a single activation function is used on all neurons. â†© It is a one line function, I know I know, but I can already sense there will be more to it later since this just works for a single input example. â†© Since at the moment the multi-threshold neuron model uses only a single example at a time, to make a fair comparison both DNN weights are updated on each example (batches of size 1), x, y = mnist.train.next_batch(1, shuffle=True) . â†© If you do that the accuracy and loss will be all over the place as it will be dependent on a single observation. This could make difficult to assess if the model is indeed getting better on each iteration by seeing the loss monotonically decrease. â†© You don't want this number to be too high since you expect an average lower loss and higher accuracy for observations at the end. If there are too many observations your standard deviation will increase and the reported average can be meaningless. â†© The weights of a neural network using relu activations where the neuron output is zero cannot learn because the back-propagated derivative is zero. â†© if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"tech","url":"/multi-threshold-neuron.html"},{"title":"Data Science with python to ATOS","text":"","tags":"workshop","url":"/2018-03-01-data-science-with-python-atos.html"},{"title":"Lead Data Scientist for Nspire project @ KPN","text":"","tags":"work","url":"/2018-03-01-data-scientist-kpn-nspire.html"},{"title":"Code breakfast deep learning edition","text":"","tags":"workshop","url":"/2018-01-17-code-breakfast-deep-learning.html"},{"title":"From PhD to GDD","text":"","tags":"talk","url":"/2018-01-09-phd-to-gdd-vrij-universiteit.html"},{"title":"Advanced Python Mastery","text":"","tags":"study","url":"/2017-12-04-advanced-python-mastery.html"},{"title":"Advanced Deep Learning","text":"","tags":"study","url":"/2017-12-02-advanced-deep-learning.html"},{"title":"\"I Pity the fool\", Deep Learning style","text":"This post was originally published in the GoDataDriven blog With deep learning applications blossoming, it is important to understand what makes these models tick. Here I demonstrate, using simple and reproducible examples, how and why deep neural networks can be easily fooled. I also discuss potential solutions. Several studies have been published on how to fool a deep neural network (DNN). The most famous study, which was published in 2015 used evolutionary algorithms or gradient ascent to produce the adversarial images. 1 A very recent study (October 2017) revealed that fooling a DNN could be achieved by changing a single pixel. 2 This subject seems fun and all but has substantial implications on current and future applications of deep learning. I believe that understanding what makes these models tick is extremely important to be able to develop robust deep learning applications (and avoid another event like random forest mania). 3 A comprehensive and complete summary can be found in the When DNNs go wrong blog, which I recommend you to read. All these amazing studies use state of the art deep learning techniques, which makes them (in my opinion) difficult to reproduce and to answer questions we might have as non-experts in this subject. My intention in this blog is to bring the main concepts down to earth, to an easily reproducible setting where they are clear and actually visible. In addition, I hope this short blog can provide a better understanding of the limitations of discriminative models in general. The complete code used in this blog post can be found here . Discriminative what? Neural networks belong to the family of discriminative models, they model the dependence of an unobserved variable (target) based on observed input (features). In the language of probability this scenario is represented by the conditional probability and it is expressed as: $$p(target|features)$$ it reads: the probability of the target given the features (e.g. the probability that it will rain based on yesterday's weather, temperature and pressure measurements). Multinomial logistic regression models are also part of these discriminative models and they basically are a neural network without a hidden layer. Please don't be disappointed but I will start by demonstrating some concepts using multinomial logistic regression. Then I'll expand the concepts to a deep neural network. Fooling multinomial logistic regression As mentioned before a multinomial logistic regression can be seen as a neural network without a hidden layer. It models the probability of the target ( \\(Y\\) ) being a certain category ( \\(c\\) ), as a function ( \\(F\\) ) that depends on the linear combination of the features ( \\(X=(X_1, X_2,...,X_N)\\) ). We write this as $$P(Y=c|X)=F(\\theta_{c}&#94;T\\cdot X)$$ where \\(\\theta_c\\) are the coefficients of the linear combination for each category. The predicted class by the model is the one which gives the highest probability. When the target \\(Y\\) is binary, \\(F\\) is taken to be some sigmoid function , the most common being the logistic function . When \\(Y\\) is multiclass we commonly use \\(F\\) as the softmax function . Apart from the conceptual understanding of discriminative models, the linear combination of the features ( \\(\\theta_{c}&#94;T\\cdot X\\) ) is what makes classification models vulnerable as I will demonstrate. In the own words of Master Jedi Goodfellow: \"Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples\". 4 Iris dataset When I was thinking on how to do this blog post and actually visualize the concepts, I concluded I needed two things: A 2-dimensional feature space. A model with high accuracy on this space. The 2-dimensional space because I wanted to generate plots which directly show the concepts. High accuracy because it's meaningless if I am able to fool a bad model. Lucky for me, it turns out that a good accuracy can be obtained on the Iris dataset by just keeping two features: petal length and petal width. Putting everything into shape this is how the data looks like This dataset contains only 150 observations, I will fit the model to all the data using a cross-entropy loss function and a L2 regularization term. This is just a plug and play from the amazing scikit-learn. model = LogisticRegression ( max_iter = 100 , solver = 'lbfgs' , multi_class = 'multinomial' , penalty = 'l2' ) model . fit ( X = iris . loc [:, iris . columns != 'flower' ], y = iris . flower ) The mean accuracy of the model is \\(96.6\\%\\) . This score is based on the training data and can be misleading, even if I am using a regularization term I can still be overfitting. 5 Let's now look at our predictions and at how our model is drawing the classification boundaries. In Figure 0 the red outer circles indicate those observations that were wrongly classified. The setosa flowers are easily identified and there is a region where the versicolor and virginica observations are close together. In Figure 1 we can see the different regions for each flower category. The regions are separated by a linear boundary, this is a consequence of the linear combination model used \\(P(Y=c|X)=F(\\theta_{c}&#94;T\\cdot X)\\) . As mentioned, in the case of a logistic regression (binary classification) \\(F\\) is the logistic function $$F(\\theta_{c}&#94;T\\cdot X)=\\frac{1}{1 + e&#94;{-\\theta_{c}&#94;T\\cdot X}}$$ and the classification boundary is given by \\(P(Y=c|X)=\\frac{1}{2}\\) when \\(\\theta_{c}&#94;T\\cdot X=0\\) . If the features are in one dimension then the boundary will be a single value, for two features the boundary is a single line and for three features a plane and so on. In our multinomial case we use the softmax function $$F(\\theta_{c}&#94;T\\cdot X)=\\frac{e&#94;{\\theta_{c}&#94;T\\cdot X}}{\\Sigma_{i=1}&#94;Ne&#94;{\\theta_{i}&#94;T\\cdot X}}$$ where the sum over \\(i\\) in the denominator runs over all the possible classes of the target. In the regions where only two classes have a non-negligible probability the softmax function simplifies to the logistic function. Therefore the linear classification boundaries between two regions is given by the contour \\(P(Y=c|X)=\\frac{1}{2}\\) as shown in Figure 3. In addition, when none of the classes have a negligible probability the boundary approaches the contour \\(P(Y=c|X)=\\frac{1}{3}\\) where the uncertainty of our prediction is maximum. This region is illustrated in Figure 2. A thing to note is that the regions extend to values which can be very far from the observations, which means we can grab a petal length of 1 and petal width of 4 and still be classified as a setosa. Even more, Figure 4 shows that even far away from our observations we can find regions with extremely high probability. We can even use a negative petal length! Let's pick some points from Figure 4 and see if I am able to fool the multinomial logistic classifier: Point: (.1, 5) Prediction: setosa Probability: 0.998 Point: (10, 10) Prediction: virginica Probability: 1.0 Point: (5, -5) Prediction: versicolor Probability: 0.992 The three points give a high probability on the prediction but are not even remotely like the observations in our dataset. Ok, now to the good stuff. Fooling a Deep Neural Network As I said before, in order for me to demonstrate the concepts and have a comprehensive visualization I need two things A 2-dimensional feature space. A model with high accuracy on this space. In the case of a deep neural network it makes no sense to attack a problem with 2 features, as the intent of neural network is to throw a bunch of features as the input layer and let the hidden layers figure out and construct new features which are relevant to my classification problem. So my reasoning as how to solve my first requirement goes as follows: Build a DNN where the last hidden layer has two units. Then do the space analysis on the features from that layer. Pick a point on that layer space which is far from the propagated observations but still is classified with a high probability. Invert all the operations made from the input layer to that last hidden layer and apply them to my selected 2D point from step 3. If I can perform those steps I should end with an input which is nothing like my observations but still is classified with high probability by the DNN, giving me an adversarial example. MNIST I chose the MNIST digits since it is a straight forward dataset to perform classification and it is complex enough to apply a DNN. I only take 4 classes, the numbers \\({0, 1, 2, 3}\\) . The final dataset consists of a bit more than 28,000 observations with 28x28=784 features. digits = fetch_mldata ( \"MNIST original\" ) index = np . in1d ( digits . target , [ 0 , 1 , 2 , 3 ]) digits . data = digits . data [ index ] digits . target = digits . target [ index ] Number of observations: 28911 Nr. observations per class: 1.0 7877 3.0 7141 2.0 6990 0.0 6903 A sample view of our observations: DNN configuration The challenge here is to find the correct configuration such that the training of the DNN converges and has a good performance on the training set. In addition, in order to be able to invert all the operations from the input layer to the last hidden layer then all functions applied must have an inverse. This means that if I decide to use any of the activation functions provided: logistic, tanh and relu, I need to keep track and impose restrictions on my nodes activation so that they are in the codomain of the activation function. This is not trivial and in my opinion does not add much to the concepts I'm trying to get across. Therefore I use the identity activation which can make the convergence a bit more tricky. 6 The final configuration of the DNN consists of: 3 hidden layers with sizes {50, 20, 2}. Identity activation function (no activation function). Stochastic gradient descent optimizer (sgd). Adaptive learning rate. dnn_identity = MLPClassifier ( hidden_layer_sizes = ( 50 , 20 , 2 ), activation = 'identity' , solver = 'sgd' , learning_rate = 'adaptive' , learning_rate_init =. 00005 , random_state = 21 ) The DNN achieved close to 95% accuracy and reached conversion quite nicely as shown in Figure 6. For comparison and for use in my arguments I built another DNN with an activation function \\(tanh\\) using the Adam optimizer. dnn_tanh = MLPClassifier ( hidden_layer_sizes = ( 50 , 20 , 2 ), activation = 'tanh' , solver = 'adam' , learning_rate_init =. 0001 , random_state = 21 ) The second DNN with the activation function achieved an accuracy of 98%, the loss curve in Figure 7 reveals that the training can be further improved but for now this is good enough. Extract feature encoding from the last hidden layer Once the model is trained we can retrieve the coefficients connecting all the layers. We use these coefficients to \"manually\" propagate our observations input up to the last hidden layer and then plot some of them in a 2D graph. This small function propagates the input layer up to a specified layer. def propagate ( input_layer , layer_nr , dnn , activation_function ): \"\"\"Obtain the activation values of any intermediate layer of the deep neural network.\"\"\" layer = input_layer for intercepts , weights in zip ( dnn . intercepts_ [: layer_nr ], dnn . coefs_ [: layer_nr ]): layer = activation_function ( layer . dot ( weights ) + intercepts ) return layer hl_identity = propagate ( digits . data , 3 , dnn_identity , lambda x : x ) hl_tanh = propagate ( digits . data , 3 , dnn_tanh , np . tanh ) The representation of the observations under the encoding of the last 2D hidden layer is shown on Figure 8 and 9. The identity DNN, as shown in Figure 8, has encoded our observations by creating hidden features which separate them in the hidden layer dimensionality (2D in this case). The more sophisticated \\(tanh\\) DNN achieves better performance because it is capable of coming up with hidden features which separate in a better way our observations as shown in Figure 9. Nevertheless Figures 10 and 11 reveal that in both cases linear classification boundaries are being constructed to separate our category regions. Similar to the multinomial logistic regression, this is caused by the dot product (linear Kernel) between the last hidden layer and the final weights which connect the hidden layer with the output layer. This means that these regions extend far away from where our observations lie, even more these regions have a high probability as shown in Figure 12 and 13. So now the only thing to do is to grab a point from Figure 8 (for example: -200, 200), do all the inverse operations to bring back the encoding to the input layer and reshape the vector into an image which of course will look nothing like a \\(1\\) but will be classified as a \\(1\\) with very high probability by our DNN. Brief tangent Before proceeding I would like to have a more conceptual discussion regarding the implications of the arguments presented for figure 8 and 9. The DNN creates hidden features which separate our observations as best as possible. This means that such hidden features will concentrate on capturing differences between our classes. For example, let's say we want to classify dogs and horses 7 . According to our reasoning, will a feature that captures the amount of legs be created? I don't think so, because having such a feature doesn't add to the purpose of separating our classes. We can send a horse with 5 legs and this fact will not raise any flags on our DNN. I believe this is the underlying concept when we say that discriminative models do not capture the essence of the objects to be classified. Here is where generative models come to the rescue, they recently have shown amazing results by capturing the underlying \"context\" of the objects. In a probability framework they shift from modelling the conditional probability to model the joint probability. Notice that the probability near the boundaries grows exponentially with the product \\(\\theta_c\\cdot X\\) following the sigmoid function. This means that if we take an observation which lies close to a boundary, it takes a small perturbation to take it to another region. This is the principle behind the study of fooling a DNN with a single pixel change 2 . Finally notice that all our analysis is in a 2D space and as such the regions extend in a surface. In a 3D space these regions will become volumes, hence increasing the region size where adversarial examples can be found. Just like Master Jedi Goodfellow said: \"Linear behavior in high-dimensional spaces is sufficient to cause adversarial examples\" 3 . Pity the fool A bit of linear algebra. Two consecutive layers can be described by a set of linear equations which in matrix notation can be represented by 8 : $$L_{N}&#94;i\\times \\Theta_{N\\times M}=L_{M}&#94;{i+1}$$ where \\(i\\) is a certain layer number, \\(N\\) and \\(M\\) the number of units in the layer and \\(\\Theta\\) the coefficients representing the connections between layers. In our DNN each layer reduces in size, this means that \\(N>M\\) . In order to find the layer \\(i\\) from the layer \\(i+1\\) we need to find the inverse of \\(\\Theta_{N\\times M}\\) and compute $$L_{N}&#94;i=L_{M}&#94;{i+1}\\times \\Theta_{N\\times M}&#94;{-1}$$ The problem (of course) is that non-square matrices do not have an inverse. In the DNN context, what is happening is that we are losing information by compacting our observations in a lower dimensional space. This means there is no way to exactly trace back layers, simply because we don't have enough information. This does not mean that we cannot find a vector representing \\(L&#94;i_N\\) which satisfies \\(L_{N}&#94;i\\times \\Theta_{N\\times M}=L_{M}&#94;{i+1}\\) given the layer \\(L&#94;{i+1}_M\\) and the coefficients \\(\\Theta_{N\\times M}\\) , which means that such vector is not unique. A solution for the layer \\(L_{N}&#94;i\\) can be derived using the pseudoinverse, in particular the Mooreâ€“Penrose inverse is adequate for our type of problem, and best of all it is implemented in Numpy! Below I define a function which inverts the propagation from a hidden layer to the input layer with an identity activation function. 9 def invert_propagation ( hidden_layer , layer_nr , dnn ): \"\"\"Obtain the input layer from a hidden layer of a deep neural network\"\"\" layer = hidden_layer for intercepts , weights in zip ( nn . intercepts_ [ layer_nr :: - 1 ], nn . coefs_ [ layer_nr :: - 1 ]): inv_weight = np . linalg . pinv ( weights ) layer = ( layer - intercepts ) . dot ( inv_weight ) return layer Finally, the moment of truth. I choose a nonsense value for each region by looking at Figures 10 and 12. In particular I choose: Region 0: (1200, -300) Region 1: (-500, 500) Region 2: (100,400) Region 3: (-1000, 900) Now I invert the propagation for each point, obtain the input layer, reshape the input to a 28x28 image and show it together with the prediction from the DNN and the probability of such prediction. def pity_the_fool ( hidden_vector , dnn , ax ): input_vector = invert_propagation ( hidden_vector , 2 , dnn_identity ) ax . imshow ( input_vector . reshape (( 28 , 28 )), cmap = 'gray' ) prediction = dnn . predict ( input_vector . reshape ( 1 , - 1 ))[ 0 ] probability = np . max ( dnn . predict_proba ( input_vector . reshape ( 1 , - 1 ))) ax . set_title ( \"Prediction: {:.0f} \\n \" \"Probability: {:.3f} \\n \" \"Hiden vector: {}\" . format ( prediction , probability , hidden_vector )) ax . axis ( 'off' ) The figures above clearly show that I have managed to fool the DNN. It is like the DNN had some Mexican peyote or something. The labels are consistent with the regions we took the points from and are classified with almost 100% probability. There is no way a human eye can tell that those images are a 0, a 1, a 2 and a 3. Not even to tell that there are numbers. Light at the end of the tunnel I have stated that the main problem is the linear classification boundaries, is there a way we can avoid this? Well, I left a hint out there when I mentioned that the dot product presented is nothing more than a linear kernel. I will not go into the details of how the kernel trick works, but in summary it lets us perform dot products in higher dimensional spaces of our features without ever computing the new features in that high-dimensional space. If you never heard about it, it can be a bit of a weird thing. Just to mess more with your cerebro, if for example we were to use the Gaussian kernel , this is equal to performing calculations in an infinite high-dimensional space, yes infinite! 10 By using these kernels the model is not restricted to linear classification boundaries. Below I compare a support vector machine model (SVM) with a linear kernel and a Gaussian kernel using the iris dataset. svm_linear = SVC ( kernel = 'linear' , probability = True ) svm_gaussian = SVC ( kernel = 'rbf' , probability = True ) Both SVM models obtain an accuracy of \\(\\approx 96.6%\\) . Figure 18 shows that the SVM with the linear kernel also suffers from the issues discussed. In general any discriminative model that is trying to model the conditional probability via some transformation of the dot product \\(\\Theta\\cdot X\\) is doomed to be susceptible to adversarial examples attacks. Figure 19 is beautiful, shows exactly how getting rid of the linearity ( \\(\\Theta\\cdot X\\) ) allows for non-linear classification boundaries and hence the regions with high probability do not extend indefinitely. In this case all points with high probability are close to our observations, so in principle they should \"look\" like our observations. A SVM with a Gaussian kernel can't accomplish the extremely complicated tasks that deep neural networks can, but an idea could be to find a way to implement a non-linear kernel between the last hidden layer and the output layer. This discussion is outside the of scope of this article, but hopefully I will find the time to look into it and write about my findings. Another solution to the above discussed issues lies in a completely different perspective, instead of trying to model the conditional probability, try to model the joint probability with generative models. These models should capture the underlying \"context\" of our observations and not only what makes them different. This fundamental difference allows generative algorithms to do things which are impossible for a DNN. Such as producing never seen examples which have a striking resemblance to original observations, and even more to tune the context of these examples. A super nice demonstration is the generation of never seen faces where the degree of smiling and sunglasses is tuned. Adios Well that took much more work than I expected. I hope you enjoyed reading this blog post and got excited about deep learning. You can find the code here . If you have any other questions just ping me in twitter @rragundez . Deep Neural Networks are Easily Fooled â†© One pixel attack for fooling deep neural networks â†© â†© This should be the case not only for Deep Learning models but all models in general. I increasingly see pseudo Data Scientist making outrageous claims or using models with a one-fits-all mentality. I understand there are juniors in the organizations but that's why you should have a strong Lead Data Scientist to provide guidance or hire GoDataDriven to make your team blossom, not only on their technical abilities but also in their mentality when attacking a problem. â†© â†© Explaining and harnessing adversarial examples â†© For the demonstration I decided to train on all the data since the dataset is so small (150 observations). In the deep neural network case I will use a much larger dataset and a test set. â†© It is known that no activation function can lead to exploiting activations values which in turn affect the convergence of the Deep Neural Network. â†© I don't like cats. â†© This is taking into account the intercept into the coefficients and adding a unit to layer \\(i\\) with an activation of 1. â†© If you were to use an activation function here is where you need to be careful that activation stay in the codomain of activation function. Since we cannot exactly reconstruct the previous layer we cannot be sure that the pseudo inverse will yield values which are outside the codomain therefore generating an exception. I tried a little bit with the tanh activation function but at least for me it was not straight forward. â†© This is because of the Taylor expansion of the exponential function. â†© if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"tech","url":"/fool-neural-network.html"},{"title":"Senior Data Scientist @ Unilever","text":"Rodrigo worked developing a fuzzy name matching algorithm for entity data of all countries within the Unilever market. Data coming from different sources has been unavoidably duplicated. The goal was to create golden records which will be enriched from all matching ones. This is a known problem since already with 1 million records yields a cartesian product of 10&#94;12 comparisons, which is unfeasible. By vectorizing the names, Rodrigo has built a Machine Learning approach which yield a run-time of 1.5hrs for ~10 million records across 50 countries.","tags":"work","url":"/2017-11-01-data-scientist-unilever.html"},{"title":"Machine Learning Application Skeleton","text":"In this blog post I provide an overview of a Python skeleton application I made. This skeleton can help you bridge the gap between your model and a machine learning application. For example, you can use your existing Flask application, import it in run_app.py as app , and this will add the production ready features of Gunicorn . Take me to the code Why bother? The times when business saw machine learning models as black boxes with no hope of understanding are long gone. It use to be that the data analytics or data science department of a company produced results in a silo kind of environment. Little or no interaction took place between these departments and the business side making the decisions (marketing, sales, client support, etc.). Advice coming from machine learning models consisted of reports, which were nice to have if they supported ideas from the business. As data driven decisions demonstrated their value, the business side started peeking behind the curtain. Paper/files reports have been substituted by static reporting dashboards, which themselves are being replaced by interactive ones. The business end users want to interact with the models, understand why certain predictions are made and evenmore, they want to be capable of performing predictions on the fly (imagine simultaneously having a customer on the phone and updating the probabilities of him/her buying certain products, or a marketing department tuning campaigns themselves depending on regional features). In short, I had some time during a rainy weekend and a GDD Friday 1 , already did something similar for a client and I think it is important to bring machine learning models to the business side. Also, as a bonus they will stop bothering you every time they need insights or a slightly different prediction. What's in the goody bag? Template to extend a Flask application using Gunicorn . This allows the application to be run in a more production ready environment (multiple workers and threads for example). In here you can find a complete list of all the possible Gunicorn settings. I added the possibility to use some of them as command line arguments. Some relevant ones are: host port workers - define number of workers. threads - number of threads on each worker. daemon - run application in the background. access-logfile - save access logs to a file. forwarded-allow-ips - list allowed IP addresses. Dummy application which demonstrates how to ingest several types of user inputs into your Python application. Debug mode which (similar to Flask) will run a single process logging to debug level restart process on code change reload html and jinja templates on change Dockerfile template to containerize the application. Interactive application which runs a classifier model, outputs predictions and information about the machine learning model. The model can be run by using the UI or by directly making a post request to the endpoint. A more complete description, a set of instructions and the code can be found in this repository . Note: I also include a setup.py file that you should use to install your package used in the application. Adios If you structure your project following the advice from Henk Griffioen (A.K.A. El Chicano), the integration of this ML application skeleton to your project should be straight forward. I hope this work can help you bring your models into a machine learning application, it certainly helped and will help me in the future. You can find the code here . If you have any other questions just ping me in twitter @rragundez . One Friday a month when we get to do whatever we want, it is awesome. â†©","tags":"tech","url":"/ml-pyapp.html"},{"title":"GoDataDriven Go video","text":"","tags":"extras","url":"/2017-07-01-godatadriven-go.html"},{"title":"Senior Data Scientist @ KPN","text":"Rodrigo worked in the commercial analytics department of KPN (Telecom). He had several roles, he was doing the work of a Senior Data Scientist by giving advice and direction to several projects, at the same time he helped the analytics team build better models. In addition he built a tool where analysts could easily extract insights from the predictions, specially for the predictive drivers of classification models. Such insights were latter used to guide marketing campaigns.","tags":"work","url":"/2017-07-01-data-scientist-kpn.html"},{"title":"Senior Data Scientist @ Nederlandse Energie Maatschappij","text":"Rodrigo worked as the senior data scientist optimizing an existing shelf-replenisher prediction system. He also developed a customer predictive algorithm to use as an input to improve the shelf-replenisher predictions. In addition he migrated all scheduled jobs to Airflow, and helped built a dashboard to monitor the model performance in production.","tags":"work","url":"/2017-05-01-data-scientist-nle.html"},{"title":"Facebook's Prophet: Forecasting Stores Transactions","text":"This post was originally published in the GoDataDriven blog Yesterday Giovanni , our Chief Scientist, mentioned this recently released (2 days ago in github ) open source forecasting API by Facebook's Core Data Science team, so I decided to give it a try during one of our famous GDD Fridays. In Prophet's own words: \" Prophet is a procedure for forecasting time series data. It is based on an additive model where non-linear trends are fit with yearly and weekly seasonality, plus holidays. It works best with daily periodicity data with at least one year of historical data. Prophet is robust to missing data, shifts in the trend, and large outliers\". Prophet's algorithm explanation can be found in this article . Prophet offers a R and Python API, I used the Pythton API of course. Why bother? The data belongs to a customer for which models are alreading in production. I wanted to see how Prophet's forecasts behave using the same data we use in one of these models developed by Rogier and me. In reality, the forecast of the number of transactions in a shop is used as a part of an ensemble to predict products sales. Since Prophet does not accept features, it would be unfair to make a comparison at that level since, for example, price is a very important factor. Data: transactions and holidays The data is of a current client, therefore I won't be disclosing any details of it. Our models make forecasts for different shops of this company. In particular I took 2 shops, one which contains the easiest transactions to predict from all shops, and another with a somewhat more complicated history. The data consists of real transactions since 2014. Data is daily with the target being the number of transactions executed during a day. There are missing dates in the data when the shop closed, for example New Year's day and Christmas. The holidays provided to the API are the same I use in our model. They contain from school vacations or large periods, to single holidays like Christmas Eve. In total, the data contains 46 different holidays. Code If the data is in a nice format (this is a big if), Prophet provides a very easy to use API. In particular, once I cleaned, aggregated and dumped the data, the calculation consisted on these two pieces of code: def predict ( tseries , predict_date , holidays = None ): model = Prophet ( holidays = holidays ) # train on data until 3 days before model . fit ( tseries [ tseries . ds < ( predict_date - timedelta ( days = 2 ))]) forecast = model . predict ( model . make_future_dataframe ( periods = 5 )) return forecast . loc [ forecast . ds == predict_date , [ 'ds' , 'yhat' ]] pred = [] pred_holidays = [] for date in pd . date_range ( '2016-1-1' , '2016-12-31' ): pred . append ( predict ( tseries_shop , date )) pred_holidays . append ( predict ( tseries_shop , date , holidays = holidays )) predictions = pd . merge ( pd . concat ( pred ), pd . concat ( pred_holidays ), on = 'ds' , how = 'inner' , suffixes = ( '' , '_hol' )) The forecast is done for 2016 with and without holiday data. Our production model gets trained daily via an Airflow job, to make a fair comparison, I train a Prophet model for each date in 2016 using the data until 3 days before the date to be forecast. This is because the order for a product needs to be submitted 2 days before, which means it uses the data available until then. Prophet leveraged the full capacity of my laptop using all 8 cores. The calculation took around 45 minutes per shop, which means a single day with or without holidays takes around 4 seconds. Metric The metric I used to measure the forecast performance is the coefficient of determination ( \\(R&#94;2\\) score) . The \\(R&#94;2\\) score gives the proportion of the variance in the data that is explained by the forecast. A perfect forecast will give 1.0 and a constant prediction for every day will give 0.0. Easy shop: Widushop Using Vincent's awesome Pokemon name generator , I will call this shop Widushop. This is the transaction data for the 3 years, The image shows a very similar pattern each year. Also, it shows some days that are definitely holidays where transactions drop or increase dramatically. Prophet produces a very accurate forecast, it scores 0.89 without using holidays and 0.94 using holidays. Below I show a comparison between the transactions (truth) and the forecast using holidays. Pretty nice! Overall it produces very good results, for holidays seems to overestimate (look at Christmas Eve), nevertheless that can be tuned by the parameter holidays.prior.scale as stated in the documentation . Difficult shop: Qumashop This time the shop name generated is Qumashop. The transaction history of Qumashop is more chaotic than the one for Widushop. Below I show the transaction history of Qumashop. Holidays have a much greater impact. Look at that peak in the middle of July, this is a known event that draws a lot of people to the city (it is in the holidays data). Notice that transactions in 2016 are considerably higher than other years, specially from July until September. Not catching this uprise trend would mean losing a lot of potential sales. This time the Prophet forecast is not as good as for Widushop giving 0.64 without holiday data and a solid 0.82 using holidays. Below I show a comparison between the transactions (truth) and the forecast using holidays for Qumashop. Look at that! very nice. I am specially happy that it catched the mentioned trend between July and September. Moreover, the residuals on the week following the big peak in July, the second week in September and the two weeks at the end of October are too high. Remember that in practice this is just a model of an ensemble, is better to have a little overall bigger residual that can reduced by other models, than having weeks with such big errors. Perhaps the forecasts for the week after the big peak in July can improve by introducing a changepoint the last day of the peak holiday week. Wrap-up Prophet's out of the box results were impressive. The quality of the forecasts are comparable with those from our current model in production for these 2 shops. Calculations were parallelized over all 8 cores of my machine. Training plus prediction time for each date was about 4 seconds. The API is ridiculously easy to use and the documentation seems sufficient. For what I can read in the documentation , Prophet does not accept features. Nevertheless, Prophet's forecasts can be part of an ensemble that produces predictions with a higher granularity. It would be interesting to make a comparison for every shop. I was surprised by the result on the difficult shop history. There are also several hyperparameters that would be interesting to look into, among several, these in particular: cap : the maximum possible value of the target. changepoint : indicate where do we expect an abrupt change in the time series. changepoint_prior_scale : related to how strongly should the model adjust to trends. holidays_prior_scale : adjust the importance of holiday effects. interval_width : sets the uncertainty interval to produce a confidence interval around the forecast. This could be very useful for monitoring the quality of the forecast. Defaults to 80%. To anyone starting a project using time-series for forecasting, I really recommend taking a close look at this tool. Great work Prophet! I hope this blog has been helpful and please bother me @rragundez with your results of playing around with Prophet. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"tech","url":"/prophet-quicklook.html"},{"title":"Data Scientist / Engineer @ Knab","text":"Rodrigo worked in the insurance department building and productionizing a ranking algorithm. This model matches the best insurances to the customer needs and characteristics. He is wrapping the model in an API to make it accessible to a future application in production, this implies continuous integration and automation of ETL processes. As a second project he built and automated ETL and cross-reference process that required encryption due to security measures.","tags":"work","url":"/2017-02-01-data-scientist-engineer-knab.html"},{"title":"Data Scientist @ Bakkersland","text":"Rodrigo worked as the lead data scientist optimizing an existing shelf-replenisher prediction system. He also developed a customer predictive algorithm to use as an input to improve the shelf-replenisher predictions. In addition he migrated all scheduled jobs to Airflow, and helped built a dashboard to monitor the model performance in production.","tags":"work","url":"/2016-08-01-data-scientist-bakkersland.html"},{"title":"Data Scientist @ GoDataDriven","text":"Apart from working with customers Rodrigo has been involved in the development of the GoDataDriven accelerator training program. He has been the trainer of several topics: data science with python, making things scale, time-series and deep learning.","tags":"work","url":"/2016-07-04-data-scientist-godatadriven.html"},{"title":"Data Science Summit Europe","text":"","tags":"workshop","url":"/2016-06-06-data-science-summit-europe.html"},{"title":"Seminar Data Science & Sports 2016","text":"Tools for Predicting Sports Outcomes based on Rankings https://www.universiteitleiden.nl/nieuws/2016/04/data-science-and-sports-a-smart-combination","tags":"talk","url":"/2016-04-07-seminar-data-science-and-sports.html"},{"title":"PyData Amsterdam","text":"https://pydata.org/amsterdam2016/schedule/presentation/21/index.html Building a face recognition system with OpenCV Description In this tutorial we will create a face recognition application from scratch, it will provide you hands-on experience on the basics of Face Recognition. We will use the OpenCV library which makes the tutorial accessible to beginners. Together, we'll go from building our face dataset to recognizing faces in a live video. If time permits we will use this face recognition system to classify banking da Abstract Building a live face recognition system in the blink of a very slow eye In this hands-on tutorial we will build a live face recognition system from scratch with the use of the OpenCV methods. Since face recognition is the main goal of this tutorial we will form teams of 2-3 people and recognize the faces in a live feed. We will make use of the OpenCV computer vision and machine learning library. OpenCV includes a comprehensive set of both classic and state-of-the-art computer vision and machine learning algorithms. These algorithms can be used to: Detect Faces Recognize Faces Identify Objects Classify human actions in videos Track camera movement Track moving objects Extract 3D models of objects Produce 3D point clouds from stereo cameras Stitch images together to produce a high resolution image of an entire scene Find similar images from an image database Remove red eyes from images taken using flash Follow eye movements OpenCV is a great tool to have in hand when dealing with data problems related to media. In the case you want to create your own tuned algorithm, due to its simplicity it lets you use the majority of your resources on developing the algorithm itself and not on the manipulation of the data, which can be a pain in the â€¦ . OpenCV is not limited to Python but has C++, C, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS. Syllabus Basics of image and video manipulations Let's take a picture OpenCV and Pyplot formats: GBR vs RGB Let's take a video Write and read picture from file Detecting faces Using OpenCV methods to recognize faces in a video Draw output rectangle to recognize face Let's take a video Extract the face detected Build our data set Defining image and video manipulation classes Normalizing the dataset Creating the directory skeleton with our data Take pictures of each person in the team Train the face recognition algorithm Brief in-depth description of algorithm Use dataset to train classification algorithm Recognize faces in a live video feed Apply trained model on detected face in live feed Remarks on other OpenCV face recognition methods Playing with the OpenCV face recognition algorithm on banking data Requiered Packages to follow hands-on cv2 (OpenCV) Numpy os matpotlib sys time IPython.display To get the most of the tutorial is highly recommended to have OpenCV installed since compilation from source is required. See you there!","tags":"talk","url":"/2016-03-12-pydata-amsterdam.html"},{"title":"Data Scientist @ Leiden University / Infostrada","text":"Rodrigo was the lead data scientist in the development of a machine learning algorithm on top of a multiplayer Elo ranking system to assess the expected performance of athletes in different sports. The algorithm was capable of calculating probabilities of outcomes in matches, predicting placings in tournaments and identificating future sport talents.","tags":"work","url":"/2016-01-01-data-scientist-leiden-university-infostrada.html"},{"title":"Data Scientist @ Qualogy","text":"Most of the time Rodrigo worked at the client, internally he took the role of presenting what the newly data science department was about to the rest of the company. He also created with two more colleagues a Cloudera Hadoop workshop which was given during a week to IT employees from other departments. He also worked on the development of a face recognition system in the context of a smart-office. He was the core developer of the machine learning algorithm and also worked on cleaning and preparing the image data. He also contributed to a user interface prototype.","tags":"work","url":"/2015-12-01-data-scientist-qualogy.html"},{"title":"PhD in Theoretical Physics","text":"","tags":"study","url":"/2015-12-01-phd-theoretical-physics.html"}]}